{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /home/dat/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/dat/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "from itertools import islice\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn import svm\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = pickle.load(open('lstm_model.sav', 'rb'))\n",
    "naive_bow_model = pickle.load(open('naive_bow.sav', 'rb'))\n",
    "naive_tfidf_model = pickle.load(open('naive_tfidf.sav', 'rb'))\n",
    "svm_bow_model = pickle.load(open('svm_clf-bow.sav', 'rb'))\n",
    "svm_tfidf_model = pickle.load(open('svm_clf_tfidf.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_time(t1, t2):\n",
    "    diff = t2 - t1\n",
    "    mins = int(diff / 60)\n",
    "    secs = round(diff % 60, 3)\n",
    "    return str(mins) + \" mins and \" + str(secs) + \" seconds\"\n",
    "\n",
    "def clean_str(sentence):\n",
    "    # Remove HTML\n",
    "    review_text = BeautifulSoup(sentence, features=\"html.parser\").text\n",
    "    # Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z\\s\\s+]\", \"\", review_text).strip()\n",
    "    return letters_only\n",
    "\n",
    "\n",
    "\n",
    "def get_data(file_name):\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"-- \" + file_name + \" found locally\")\n",
    "        df = pd.read_csv(file_name)\n",
    "    return df\n",
    "\n",
    "def review_to_words(review):\n",
    "    # 1. Convert to lower case, split into individual words\n",
    "    words = review.lower().split()\n",
    "\n",
    "    # 2. Get english stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # 3. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return \" \".join(meaningful_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from file\n",
    "reviews = pd.read_csv(\"clean_train_reviews.csv\", nrows=20000)\n",
    "# ignore all 3* reviews\n",
    "reviews = reviews[reviews[\"score\"] != 3]\n",
    "# positive sentiment = 4* or 5* reviews (sentriment = True)\n",
    "reviews[\"sentiment\"] = reviews[\"score\"] >= 4\n",
    "\n",
    "# X = reviews['text'].values.astype('U')\n",
    "X = reviews['text']\n",
    "y = reviews['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_bow = CountVectorizer(analyzer=\"word\",\n",
    "                            preprocessor=None,\n",
    "                            stop_words=None,\n",
    "                            max_features=1000)\n",
    "\n",
    "vect_tfidf = TfidfVectorizer(analyzer=\"word\",\n",
    "                                preprocessor=None,\n",
    "                                stop_words=None,\n",
    "                                max_features=1000)\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_bow.fit(X.values.astype('U'))\n",
    "vect_tfidf.fit(X.values.astype('U'))\n",
    "tokenizer.fit_on_texts(X.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "senten = [\n",
    "        \"I'll start by saying I love exploring non-alcoholic options, so I wasn't expecting it to taste like gin, just to offer an interesting botanical profile and fun mixing options (I'm an athlete and I just don't like to drink most of the time). That said, this one is really vile. I see what they're trying to do, but it tastes like pine floor cleaner with a strong hot pepper burst for that real kick-you-when-you're-down painful finish. I think they're trying to mimic the 'burn' of alcohol? But it's not necessary. The flavors are also not balanced well, for all their claims of thoughtful botanicals and extracts - the strongest flavor is pine, a second note of must, and a punch of hot pepper, and then I don't taste much else. It just tastes really bad, and I've had lots of other really tasty non-alcoholic mixers and substitutes that I enjoy, so just skip this one. They also don't offer any return policy, so I'm out $25 - I guess I'll use it to clean my floors.\n",
    "\",\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for x in senten:\n",
    "    se1= clean_str(x)\n",
    "    se2 = review_to_words(se1)\n",
    "    sentences.append(se2)\n",
    "\n",
    "check_lst_bow = vect_bow.transform(sentences).toarray()\n",
    "check_lst_tfidf = vect_tfidf.transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 0, 0, 1, 0, 1])"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "naive_bow_model.predict(check_lst_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 0, 1, 0, 1, 0])"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "naive_tfidf_model.predict(check_lst_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ True,  True,  True,  True,  True, False])"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "svm_bow_model.predict(check_lst_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ True, False,  True,  True,  True, False])"
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "svm_tfidf_model.predict(check_lst_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.zeros((len(sentences), 600), dtype='int32')\n",
    "for i,sen in enumerate(sentences):\n",
    "    if i > 10:\n",
    "        break\n",
    "    # print(sen)\n",
    "    for j,word in enumerate(sen.split()):\n",
    "        if j < 600 and tokenizer.word_index[word] is not None:\n",
    "            data[i, j] = tokenizer.word_index[word]\n",
    "        else: \n",
    "            data[i, j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "lstm_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitvenvvenvb2562dc476a04f9cac9510ed63e2ef42",
   "display_name": "Python 3.6.10 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}