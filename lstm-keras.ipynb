{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n[nltk_data] Downloading package stopwords to /home/dat/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/dat/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "from itertools import islice\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn import svm\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import pickle\n",
    "\n",
    "from keras.initializers import Constant\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import LSTM,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_time(t1, t2):\n",
    "    diff = t2 - t1\n",
    "    mins = int(diff / 60)\n",
    "    secs = round(diff % 60, 3)\n",
    "    return str(mins) + \" mins and \" + str(secs) + \" seconds\"\n",
    "\n",
    "def clean_str(sentence):\n",
    "    # Remove HTML\n",
    "    review_text = BeautifulSoup(sentence, features=\"html.parser\").text\n",
    "    # Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z\\s\\s+]\", \"\", review_text).strip()\n",
    "    return letters_only\n",
    "\n",
    "def convert_plain_to_csv(text_file, csv_file):\n",
    "    t0 = time.time()\n",
    "    with open(text_file, \"r\") as f1, open(csv_file, \"w\") as f2:\n",
    "        i = 0\n",
    "        f2.write(\"productId,score,summary,text\\n\")\n",
    "        while True:\n",
    "            next_n_lines = list(islice(f1, 9))  # read 9 line\n",
    "            if not next_n_lines:\n",
    "                break\n",
    "\n",
    "            output_line = \"\"\n",
    "            for line in next_n_lines:\n",
    "                if \"product/productId:\" in line:\n",
    "                    output_line += line.split(\":\")[1].strip() + \",\"\n",
    "                elif \"review/score:\" in line:\n",
    "                    output_line += line.split(\":\")[1].strip() + \",\"\n",
    "                elif \"review/summary:\" in line:\n",
    "                    summary = clean_str(line.split(\":\")[1].strip()) + \",\"\n",
    "                    output_line += summary\n",
    "                elif \"review/text:\" in line:\n",
    "                    text = clean_str(line.split(\":\")[1].strip()) + \"\\n\"\n",
    "                    output_line += text\n",
    "\n",
    "            f2.write(output_line)\n",
    "\n",
    "            # print status\n",
    "            i += 1\n",
    "            if i % 10000 == 0:\n",
    "                print(i, \"reviews converted...\")\n",
    "\n",
    "    print(datetime.datetime.now(), \"- Converting completed in\", get_run_time(t0, time.time()))\n",
    "\n",
    "def get_data(file_name):\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"-- \" + file_name + \" found locally\")\n",
    "        df = pd.read_csv(file_name)\n",
    "    return df\n",
    "\n",
    "def review_to_words(review):\n",
    "    # 1. Convert to lower case, split into individual words\n",
    "    words = review.lower().split()\n",
    "\n",
    "    # 2. Get english stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # 3. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return \" \".join(meaningful_words)\n",
    "\n",
    "\n",
    "def cleaning_data(dataset, file_name):\n",
    "    t0 = time.time()\n",
    "    num_reviews = dataset[\"text\"].size\n",
    "    clean_train_reviews = []\n",
    "\n",
    "    # Loop over each review\n",
    "    for i in range(0, num_reviews):\n",
    "        # If the index is evenly divisible by 1000, print a message\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(\"Review\", i + 1, \"of\", num_reviews, \"\\n\")\n",
    "\n",
    "        productId = str(dataset[\"productId\"][i])\n",
    "        score = str(dataset[\"score\"][i])\n",
    "        summary = str(dataset[\"summary\"][i])\n",
    "        text = review_to_words(str(dataset[\"text\"][i]))\n",
    "\n",
    "        clean_train_reviews.append(productId + \",\" + score + \",\" + summary + \",\" + text + \"\\n\")\n",
    "\n",
    "    print(\"Writing clean train reviews...\")\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(\"productId,score,summary,text\\n\")\n",
    "        for review in clean_train_reviews:\n",
    "            f.write(\"%s\\n\" % review)\n",
    "\n",
    "    \n",
    "    print(datetime.datetime.now(), \"- Write file completed in\", get_run_time(t0, time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 600\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from file\n",
    "reviews = pd.read_csv(\"clean_train_reviews.csv\", nrows=20000)\n",
    "# ignore all 3* reviews\n",
    "reviews = reviews[reviews[\"score\"] != 3]\n",
    "# positive sentiment = 4* or 5* reviews (sentriment = True)\n",
    "reviews[\"sentiment\"] = reviews[\"score\"] >= 4\n",
    "\n",
    "X = reviews['text'].values.astype('U')\n",
    "# X = reviews['text']\n",
    "y = reviews['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(18351,)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=20000)\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total 23197 unique tokens.\n"
    }
   ],
   "source": [
    "\n",
    "data = np.zeros((X.shape[0], MAX_SENT_LENGTH), dtype='int32')\n",
    "for i,sen in enumerate(X.tolist()):\n",
    "    for j,word in enumerate(sen.split()):\n",
    "        if j < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "            data[i, j] = tokenizer.word_index[word]\n",
    "        \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = \".\"\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total 400000 word vectors.\n"
    }
   ],
   "source": [
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(23198, 100)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_5 (InputLayer)         (None, 600)               0         \n_________________________________________________________________\nembedding_5 (Embedding)      (None, 600, 100)          2319800   \n_________________________________________________________________\nlstm_5 (LSTM)                (None, 200)               240800    \n_________________________________________________________________\ndense_9 (Dense)              (None, 128)               25728     \n_________________________________________________________________\ndense_10 (Dense)             (None, 1)                 129       \n=================================================================\nTotal params: 2,586,457\nTrainable params: 266,657\nNon-trainable params: 2,319,800\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "\n",
    "int_sequences_input = Input(shape=(MAX_SENT_LENGTH,), dtype=\"int32\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = LSTM(200, dropout=0.25, recurrent_dropout=0.25)(embedded_sequences)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "preds = Dense(1, activation='softmax')(x)\n",
    "model = Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((14680, 600), (3671, 600))"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((3671, 600), (3671,))"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "lb_encoder = LabelEncoder()\n",
    "y_train = lb_encoder.fit_transform(y_train)\n",
    "y_test = lb_encoder.transform(y_test)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 1544,     7,   311, ...,     0,     0,     0],\n       [   20,     5,    40, ...,     0,     0,     0],\n       [  455,  3275,   349, ...,     0,     0,     0],\n       ...,\n       [    7,    57, 16190, ...,     0,     0,     0],\n       [  300,  3904,    19, ...,     0,     0,     0],\n       [ 1044,    92,    24, ...,     0,     0,     0]], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_hinge',\n",
    "    optimizer='nadam',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 14680 samples, validate on 3671 samples\nEpoch 1/3\n14680/14680 [==============================] - 450s 31ms/step - loss: 0.3172 - acc: 0.8414 - val_loss: 0.3356 - val_acc: 0.8322\nEpoch 2/3\n14680/14680 [==============================] - 460s 31ms/step - loss: 0.3172 - acc: 0.8414 - val_loss: 0.3356 - val_acc: 0.8322\nEpoch 3/3\n14680/14680 [==============================] - 472s 32ms/step - loss: 0.3172 - acc: 0.8414 - val_loss: 0.3356 - val_acc: 0.8322\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7fcbac475128>"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=3,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'lstm_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3671/3671 [==============================] - 46s 13ms/step\nTest score: 0.33560337773690396\nTest accuracy: 0.8321983218193054\n"
    }
   ],
   "source": [
    "\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=64)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nAverage accuracy across folds: 83.22%\n\nAverage F1 score across folds: 90.84%\n\nAverage Precision score across folds: 83.22%\n\nAverage Recall score across folds: 100.00%\n\nAverage Confusion Matrix across folds: \n [[   0  616]\n [   0 3055]]\n"
    }
   ],
   "source": [
    "accs =accuracy_score(y_test, y_pred)\n",
    "f1s = f1_score(y_test, y_pred)\n",
    "cms = confusion_matrix(y_test, y_pred)\n",
    "pres = precision_score(y_test, y_pred)\n",
    "recs = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nAverage accuracy across folds: {:.2f}%\".format(accs* 100))\n",
    "print(\"\\nAverage F1 score across folds: {:.2f}%\".format(f1s * 100))\n",
    "print(\"\\nAverage Precision score across folds: {:.2f}%\".format(pres* 100))\n",
    "print(\"\\nAverage Recall score across folds: {:.2f}%\".format(recs* 100))\n",
    "print(\"\\nAverage Confusion Matrix across folds: \\n {}\".format(cms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitvenvvenvb2562dc476a04f9cac9510ed63e2ef42",
   "display_name": "Python 3.6.10 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}